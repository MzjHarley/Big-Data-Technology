|Author|Date|
|---|---|
|MZJ|3/7,2022|

>HDFS，全称为"Hadoop Distributed File System",是一种通过网络实现文件在多台主机上进行分布式存储的文件系统，主要针对大规模数据的存储而设计的，用于处理大规模文件，如TB级  
>采用"客户机/服务器"模式，客户端以特定的通信协议通过网络与服务端建立连接，提出文件访问请求，客户端和服务器可以设置访问权限来限制请求方对底层数据存储块的访问。
## 计算机集群结构

分布式文件系统**将文件分布存储到多个计算机节点，成千上万个计算机节点构成计算机集群。**  
**目前分布式文件系统采用的计算机集群为普通硬件**。这就大大降低了硬件上的开销。  
集群中的计算机节点放在机架上，机架上可以存放8 ~ 64个节点，**同一机架的不同计算机节点之间通过网络互连，不同机架之间采用交换机或路由器**。  

![contents](https://github.com/MzjHarley/Big-Data-Technology/blob/main/img/2.png)

## 分布式文件系统的结构
分布式文件系统**将文件分成若干个块进行存储**，块是数据读写的基本单元，HDFS第一代默认一个块的大小为64MB，第二代默认128MB.  
与普通文件系统不同，在分布式文件系统中，如果一个文件小于一个数据块的大小，它并不占用整个数据块的存储空间。  
分布式文件系统的物理结构上由计算机集群中的多个节点构成。节点分为两类。
|节点类型|作用|
|---|---|
|主节点|也叫名称节点，负责文件和目录的创建，删除和重命名等，同时管理着数据节点和文件块的映射关系（元数据）|
|从节点|也叫数据节点，负责存储和读取数据，据名称节点的命令创建，删除和复制数据块|

#![contents](https://github.com/MzjHarley/Big-Data-Technology/blob/main/img/4.png)

>客户端只有通过**访问名称节点才能找到请求的文件块所在位置，进而到相应位置读取所需文件块。**  
>在存储时，由名称节点分配存储位置，然后由客户端把数据写入相应数据节点。  
>在读取时，客户端从名称节点获得数据节点和文件块的映射关系，到相应位置访问文件块。  
>由于计算机集群上的节点可能发生故障，为保证数据的完整性，分布式文件系统通常采用多副本存储。  
>文件块会被复制成多个副本，存储在不同的节点上，而不同副本的各个节点会分布在不同的机架上。  
>这样在单个节点出现故障时，可以快速调用副本重启单个节点上的计算过程，而不用重启整个计算过程。整个机架出现故障时不会丢失所有文件块。
## 分布式文件系统的设计需求
|设计需求|具体含义|HDFS实现情况|
|----|----|----|
|透明性|  包括访问透明性、位置透明性、性能和伸缩透明性；访问透明性：用户不需要专门区分哪些是本地文件，哪些是远程文件。用户能够通过相同的操作来访问本地和远程文件资源；位置透明性：不改变路径名的前提下，不管文件副本数量和实际存储位置发生何种变化，对用户而言都可以通过相同的路径名访问同一个文件；性能和伸缩透明性：系统中节点的增加和减少以及性能的变化对用户而言是透明的，用户感觉不到什么时候节点加入或退出| HDFS 只能提供一定程度的访问透明性，完全的位置透明性、性能和伸缩透明性|
|并发控制|同一时间同一文件只允许一个writer上传|任何时间只允许一个程序写入某个文件|
|文件复制|一个文件可在不同位置拥有多个副本|多副本机制|
|硬件和操作系统异构性|可在不同的计算机和OS上实现同样的客户端和服务端程序|采用java语言开发|
|可伸缩性|支持文件的动态加入或退出|建立在大规模廉价机器上的分布式文件系统集群具有很好的伸缩性|
|容错|保证文件服务在服务端或用户端出现问题时能正常使用|多副本机制，故障自动检测和自动恢复机制|
|安全|保障系统安全性|安全性较弱|
## HDFS特点
硬件出错在普通服务器集群中是一种常态并非异常，因此HDFS在设计上采取了多种机制保证硬件出错的情况下数据的完整性。
|特点|机制|
|---|---|
|兼容廉价的硬件设备|快速检测硬件故障和进行自我恢复的机制|
|流式数据读写|HDFS是为了满足批量数据处理的要求而设计的，因此为了提高数据的吞吐率，放松了POSIX（可移植操作系统接口标准）的要求，从而以流式方式访问文件系统数据，即通过API打开文件的某个数据块后，能够顺序读取；或者通过API向HDFS写入数据。我觉得可以这样理解，就是通过java流式读取或写入。|
|大数据集|HDFS文件通常可以达到GB甚至TB级别|
|简单的文件模型|一次写入，多次读取，文件一旦完成写入，关闭后无法在写入，只能被读取|
|强大的跨平台兼容性|采用java实现|
|不适合低延迟数据访问|HDFS主要针对大规模数据批量处理而设计的，采用流式数据读取，具有很高的数据吞吐率，也意味着较高的延迟|
|无法高效存储大量小文件|小文件是指文件大小小于一个块的文件|
|不支持多用户写入及任意修改文件|HFDS只允许一个文件有一个写入者，不允许多个用户对同一文件执行写操作，而且只允许追加，不可执行随机写操作|
### 为什么HDFS无法高效存储大量小文件？
+ HDFS采用名称节点管理文件系统的元数据，这些元数据被保存在内存中，从而使客户端可以快速获取文件实际存储位置。  
  如果存储大量的小文件，那么名称节点就要消耗很多的内存来保留小文件的元数据，这样元数据检索的效率就会降低，需要花费很多时间找到一个文件的存储位置。  
  如果再继续扩展到十数亿个小文件，名称节点保存元数据所需要的内存空间就会大大增加，以现有硬件水平，无法在内存中保留如此大量的元数据。  
+ 用MapReduce处理大量小文件时，会产生很多的Map任务，进程管理开销大大增加。  
+ 访问小文件的速度要远远高于访问大文件的速度，因为访问大量小文件需要不断从一个节点跳到另一个节点，严重影响性能。  

# HDFS相关概念
## 块
传统文件系统为了提高磁盘的读写效率，一般以数据块为单位，并非字节。以块为单位读写数据，可以把磁盘的寻道时间分摊到大量数据中，一旦找到第一条记录，剩下的顺序读取效率是很高的。  
HDFS同样也采用了块的概念，默认一个块为64MB.
### HDFS的块要比传统文件系统的块大得多，为什么？
+ 最小化寻址开销。HDFS的寻址开销包括磁盘寻道开销和数据块定位开销。  
  当客户端访问一个文件时，首先要从名称节点获取到组成这个文件的数据块的位置列表，然后根据这个位置列表获得实际存储的数据节点的位置，最后数据节点根据数据块信息在本地linux文件系统中找到 对应的文件，并把数据返回给客户端。  
  设计一个比较大的块，可以将寻址开销分摊到较多的数据中，降低单位数据的寻址开销。  
+ 块的大小不宜过大，MapReduce中的Map任务一次只处理一个数据块，如果启动的任务太少，会降低作业并行处理速度。
### HDFS采用块好处
+ 支持大规模文件存储。文件以块为单位进行存储，文件被分成多个文件块存储到不同数据节点上，使得一个文件的大小不受制于单个节点容量的限制，可以远远大于网络中任意节点的存储容量。  
+ 简化系统设计。首先大大简化了存储管理，文件块大小固定，可以容易的计算出一个节点可以存储多少文件块，其次方便元数据管理，元数据不需要和文件块一起存储而由其它系统管理。    
+ 适合数据备份。每个文件块都可冗余存储到多个节点，大大提高系统的容错性和可用性。
## 名称节点和数据节点
**在HDFS中，名称节点负责管理文件系统的命名空间，保存了两个核心的数据结构FsImage和EditLog。**  
**FsImage用于维护文件系统树以及文件系统中所有文件和文件夹的元数据；EditLog记录了所有针对文件的创建/删除/重命名等操作。**   
**名称节点记录了每个文件中各个块所在数据节点的位置信息，但并不持久地存储这些信息，而是在系统启动时扫描所有数据节点并重构，得到这些信息。** 
名称节点**在启动时会将FsImage中的内容加载进内存中，然后执行EditLog中的各项操作，使内存中的元数据保持最新，这个操作完成后就会产生新的FsImage文件和EditLog文件**。  
名称节点启动后并进入到正常运行状态后，HDFS中的所有更新操作都会写入到EditLog文件而非FsImage文件。这是因为对于分布式文件系统而言，**FsImage文件很大（GB级别以上），如果所有更新操作都写入到FsImage中会使系统的运行速度非常缓慢。相对来说EditLog通常要远远小于FsImage，更新操作写入到EditLog是非常高效的。**  
名称节点在启动时会进入安全模式只能提供读操作无法提供写操作，启动结束后进入正常运行状态可对外提供读写操作。  
数据节点：是HDFS中的工作节点，负责数据的读取和存储，会根据客户端或者名称节点的调度进行数据的存储和检索，并且会定期向名称节点发送自己所存储块的列表信息。每个数据块中的数据都会被保存在本地Linux文件系统中。  
## 第二名称节点
在名称节点运行期间，HDFS会不断产生更新操作，致使EditLog文件不断变大，不断变大的EditLog通常对系统的性能不会产生显著影响。  
但当名称节点重启时，需要将FsImage文件加载进入内存并执行EditLog文件中的所有操作，使FsImage文件保持最新，可想而知如果EditLog文件过于大的话，名称节点就会长时间处于安全模式，无法对外提供写操作，从而影响用户使用。  
为有效解决这个问题，HDFS在设计中采用了第二名称节点。**主要具有两个功能：完成EditLog和FsImage的合并操作，减小EditLog的大小，缩短名称节点的重启时间；作为名称节点的检查节点，保存名称节点中的元数据信息。**  
### FsImage和EditLog的合并操作。
+ 每隔一段时间，第二名称节点会和名称节点通信，请求停止EditLog文件的使用(这个时刻记作t1)，暂时将新到达的写操作添加到一个新的文件EditLog.new。  
+ 然后，第二名名称节点会将名称节点中的FsImage和EditLog文件拉回本地，再加载到内存中，对二者执行合并操作(在内存中逐步执行EditLog文件中的操作，使FsImage文件保持最新)。  
+ 合并结束后，第二名称节点会将最新的FsImage.ckpt文件发送给名称节点。  
+ 名称节点收到后，会用最新的FsImage.ckpt文件去替换旧的FsImage文件，同时用最新的EditLog.new文件替换旧的EditLog文件。(这个时刻记作t2)从而减少EditLog的大小。  
### 作为名称节点的检查点
从合并过程中可以看出第二名称节点会定期和名称节点进行通信，从名称节点获取FsImage和EditLog文件，执行合并操作后得到新的FsImage.ckpt文件。  
从这个角度可以看出第二名称节点相当于为名称节点设置了一个检查点，周期性的备份名称节点中的元数据。  当名称节点发生故障时，就会用第二名称节点中记录的元数据信息进行系统恢复。  
但第二名称节点上合并操作得到的新的FsImage文件是合并发生时(t1时刻)HDFS得元数据信息，并未包含t1 ~ t2间发生的更新操作。如果名称节点在t1 ~ t2期间发生故障，系统就会丢失部分元数据信息。  
HDFS并不支持把系统直接切换到第二名称节点，从这个角度来讲，第二名称节点只是起到“检查点”的作用，并非“热备份”的作用。  
# HDFS体系结构
HDFS采用主从结构模型，一个HDFS集群包含一个名称节点和若干个数据节点。**名称节点作为中心服务器负责管理文件系统的命名空间及客户端对文件的访问**。  
集群中的数据节点一般是一个节点运行一个**数据节点**的进程，**负责处理客户端的读写请求，在名称节点的统一调度下进行数据块的创建，删除和重命名等操作**。  
每个数据节点实际是保存在本地linux文件系统下的。他们会定期向名称节点**发送心跳信息(自己所存储块的列表信息)，报告自己的状态，没有按时发送心跳信息的数据节点会被标记为死机状态，名称不会再给分配任何I/O请求**。    
当客户端访问一个文件时，首先将文件名发送给名称节点，名称节点根据文件名找到对应数据块的信息，然后根据每个数据块的信息找到实际存储各个数据块的数据节点的位置，将数据节点位置发送给客户端，最后客户端直接访问这些数据节点获取数据。**在整个访问过程中，名称节点并不参与数据的传输，这种设计方式可以实现在不同数据节点上的并发访问，大大提高访问速度**。  
HDFS采用java语言开发，具有很强的跨平台兼容性，因此任何支持JVM的机器都可部署名称节点和数据节点。实际部署时，会选用一台性能较好的机器作为名称节点，其他机器作为数据节点。当然一台机器可以运行多个数据节点，名称节点也可以和数据节点放在同一台机器中运行。  
HDFS集群中只有一个名称节点，该节点负责所有元数据的管理，这种设计大大简化了分布式文件系统的结构，可以保证数据不会脱离名称节点的控制，同时用户的数据永远不会经过名称节点，大大减轻了名称节点(中心服务器)的负担，方便数据管理。  
## HDFS命名空间管理
命名空间包括文件，块和目录，存储在本地linux文件系统中。  
命名空间的管理是指命名空间支持对HDFS中的目录，文件和块做类似于一般文件系统的创建，修改和删除等基本操作，在HDFS体系结构中，只有一个命名空间。  
HDFS使用的是传统的分级文件体系，因此，用户可以像使用普通文件系统一样，创建、删除目录和文件，在目录间转移文件，重命名文件等。  
## 通信协议
所有HDFS通信协议都是构建在TCP/IP基础上的。  
名称节点不会主动发起RPC，而是响应来自客户端和数据节点的RPC请求。  
![contents](https://github.com/MzjHarley/Big-Data-Technology/blob/main/img/5.png)
## 体系结构的局限性
+ 命名空间的限制，名称节点保存在内存中，因此名称节点能够容纳对象的个数受到内存空间大小的限制。  
+ 性能的瓶颈，整个分布式文件系统的吞吐量受限于单个名称节点的吞吐量。  
+ 隔离问题，集群中只有一个名称节点和命名空间，无法对不同程序进行隔离。  
+ 集群可用性，一旦唯一的名称发生故障，会导致整个集群不可用。
# HDFS的存储原理
## 数据冗余存储
+ 加快数据传输速度，当多个客户端同时访问同一文件时，可以让客户端分别从不同的数据块副本中读取数据，大大加快了数据的传输速度。  
+ 容易检查数据错误，数据节点之间通过网络传输数据，采用多副本可以很容易判断数据是否出错。  
+ 保证数据的可靠性，某个数据节点失效，也不会造成数据丢失。

## 数据存取策略
### 数据存放
为了充分提高数据的可靠性与系统的可用性以及充分利用网络带宽，HDFS采用以机架为基础的数据存放策略。  
一个HDFS集群中包含很多机架，机架内部通过网络互连，机架之间通过路由器或交换机互连，这意味着机架内部通信带宽大于机架之间的通信带宽。  

>HDFS默认每个数据节点在不同的（逻辑）机架上.   
>缺点：不能充分利用同一物理机架内部机器之间的带宽；  
>优点:获得很高的数据可靠性(即使一个机架发生错误，其余机架上的数据副本仍然是可用的)  
>     可以在多个机架上进行并行读取数据，大大提高数据读取速度。  
>     可以很容易实现系统内部负载均衡和错误处理。  
>由于这个机架是通过Map映射来完成的，所以本质上是一个逻辑机架，也因此可以将不同物理机架上的节点配置在同一个逻辑机架上。  
>在实际开发过程中，为了方便管理，往往是将同一个物理机架上的节点配置在同一个逻辑机架上       

|副本放置策略|
|---|
|1.集群内发送读写请求，第1个副本放在发起请求的数据节点上，实现就近写入数据|
|2.集群外发起读写请求，则从集群内部挑选一台磁盘较为充足,CPU不太忙的数据节点，作为第1个副本的存放地|
|3.第2个副本会被放在与第1个副本不同机架上的数据节点|
|4.第3个副本会放在和第1个副本相同机架的其他节点|
|5.如果还有更多副本，则会随机选择数据节点进行存放|
## 数据读取
为了降低整体的网络带宽消耗和数据读取延迟，HDFS集群一定会让客户端尽量去读取近的副本，那么按照以上头解释的副本存放策略的结果：  
1、如果在客户端数据节点有数据，那么直接读取  
2、如果在跟客户端数据节点同机架的数据节点中有该数据块，则直接读取  
3、如果该HDFS集群跨多个数据中心，那么客户端也一定会优先读取本数据中心的数据  
HDFS是如何确定两个节点是否是同一节点，如何确定的不同服务器跟客户端的远近呢？答案就是采用机架感知。  
机架感知可以计算两个数据节点之间的距离，通过一个机架感知的过程，NameNode可以确定每一个DataNode所属的机架id.  
有了机架感知，NameNode就可以画出Datanode网络拓扑图,就可以计算出任意两台datanode之间的距离,得到最优的存放和读取策略,优化整个集群的网络带宽均衡以及数据最优分配。  
HDFS提供一个API可以确定一个数据节点所属机架ID ，客户端也可以调用API获取自己的所属机架ID，根据机架ID是否相同实现来确定读取的方式，相同则优先选择就近读取，不同则随机读取。  
## 数据复制
HDFS的数据采用了流水线复制的策略，大大提高了数据复制过程的效率。
### 当客户端向HDFS写入一个文件时
+ 这个文件首先被写入到本地，并被切分成若干个块，每个块的大小由HDFS的设定值决定。
+ 每个块都会向名称节点发起写请求，名称节点会根据系统内各个数据节点的使用情况，会返回一个数据节点列表给客户端。
+ 客户端把数据首先写入列表的第1个数据节点，同时将数据节点列表传给第1个数据节点。  
+ 第1个数据节点接收到4KB数据时写入本地，并向列表中的第2个数据发起连接请求，把自己已经接收到的数据发送给第2个数据节点.  
+ 第2个数据节点接收到4KB数据时写入本地，并向列表中的第3个数据发起连接请求，把自己已经接收到的数据发送给第3个数据节点.  
+ 以此类推，列表上多个数据节点形成了一条数据复制的流水线。最后当文件写完时，数据复制也随之完成。  
## 数据的错误和恢复
### 名称节点出错
名称节点保存着所有元数据信息，其中最核心的文件为FsImage和EditLog文件，如果这两个文件发生损坏，那么整个HDFS实例将失效。  
**HDFS采用两种机制保证名称节点的安全：一是将名称节点上的元数据信息同步保存到其他文件系统上如远程挂载的网络文件系统；二是运行一个第二名称节点**。  
当名称节点死机后可以利用第二名称节点中的元数据信息进行系统恢复，这样做会丢失部分元数据信息。  
通常将这两种机制结合使用，当名称节点死机后，会先到远程挂载的网络文件系统获取备份的元数据信息，放在第二名称节点上进行恢复，并把第二名称节点当作名称节点来使用。  
### 数据节点出错
每个数据节点会定期向名称节点发送心跳信息，向名称节点报告自己的状态。  
当数据节点发生故障或者断网时，无法向名称节点发送自己的心跳信息，就会被标记为死机，该数据节点上的所有数据会被标记为不可读，名称节点不会再给它分配任何I/O请求。  
这时，由于一些数据节点的不可读，会导致一些数据块的副本数量小于冗余因子。  
名称节点会定期检查这种情况，一旦发现某个数据块的副本数量小于冗余因子，就会启动冗余复制，为它生成新的副本。  
**HDFS系统与其他文件系统最大的区别在于可以调整冗余数据的位置。当故障时可以调整，当出现负载不均衡时也可以调整.**
### 数据出错
网络传输和磁盘错误等因素都会造成数据错误。客户端在读取数据后会根据MD5和SHA-1对数据块进行校验，以确定读取到正确的数据。  
在文件被创建时，客户端会对每一个文件块进行信息摘录并将这些信息写入到同一路径下的隐藏文件里面，当客户端读取文件时会先读取该信息文件，利用信息文件对每一个读取的文件块进行校验。  
如果校验出错，客户端就会到另一个数据节点读取该文件块，并且向名称节点报告该文件块有错误，名称节点会定期检查并复制这个块。  

|名词|解释|
|---|---|
|MD5|Message Digest Algorithm，消息摘要算法第五版，一种哈希算法，具有很强的抗修改性，原文发生一丁点变化，MD5值就会有很大的不同|
|SHA-1|Secure Hash Algorithm，安全散列算法，一种哈希算法，比MD5更加安全一点，和MD5的应用场景类似|
# HDFS数据读写过程
FileSystem是一个通用的文件系统基类，可以被分布式文件系统继承，所有使用Hadoop文件系统的代码都要使用这个类。  
Hadoop为这个文件基类提供了很多具体的实现，DistributedFileSystem就是FileSystem在HDFS中的实现。  
FileSystem中的open()方法返回一个输入流FsDataInputStream对象，在HDFS中具体的输入流就是DFSInputStream。  
FileSystem中的create()方法返回一个输出流FsDataOutputStream对象，在HDFS中具体的输入流就是DFSOutPutStream。  
## 读数据
+ 1.客户端通过FileSystem.open()方法打开文件返回FSDataInputStream，对应在HDFS中，DistributedFileSystem调用open()方法创建一个输入流DFSInputStream  
+ 2.在DFSInputStream的构造函数中，输入流通过ClientProtocal.getBlockLoations()远程调用名称节点获得文件开始部分的数据块的位置。  
    对于该数据块名称节点返回保存该数据块的数据节点的地址，同时根据距离客户端的远近进行排序；
    然后DistributedFileSystem会用DFSInputStream会实例化FSDataInputStream并返回给客户端，同时会返回该数据块的数据节点地址；  
+ 3.获取到FSDataInputStream客户端开始调用read()方法读取数据，输入流根据前面的排序结果选择距离客户端最近的数据节点建立连接并读取数据。  
+ 4.数据从数据节点读取到客户端，数据块读取完毕后FSDataInputStream关闭与该数据节点的连接。  
+ 5.输入流根据ClientProtocal.getBlockLocations()方法找到下一个数据块(如果客户端缓存中包含该数据块的信息，则不再调用)  
+ 6.找到该数据块的最佳数据节点，建立连接读取数据。  
+ 7.文件读取完毕后，调用FSDataInputStream.close()方法关闭输入流。 

在读取数据的过程中，如果客户端和数据节点通信时出现错误，就会尝试连接包含此数据块的下一个数据节点。  
## 写数据
+ 1.客户端通过FileSystem.creat()方法打开文件返回FSDataOutputStream，对应在HDFS中，DistributedFileSystem调用create()方法创建一个输入流DFSOutputStream  
+ 2.在DFSInputStream的构造函数中，输入流通过RPC远程调用名称节点在文件系统的命名空间创建一个新的文件。  
    在这过程中名称节点会执行一些检查，比如文件是否存在，客户端是否有权限创建文件等。  
    然后名称节点会构造一个新文件，并添加文件信息，远程过程调用后DistributedFileSystem会用DFSInputStream会实例化FSDataInputStream并返回给客户端  
    客户端会使用这个输出流写入数据  
+ 3.获得输出流FSDataOutputStream后，调用write()方法向HDFS问件写入数据。  
+ 4.客户端向FSDataOutputStream写入数据会被分成一个个分包放入到DFSOutputStream对象的内部队列。输出流会向名称节点申请保存文件和副本数据块的若干个数据节点。  
    这些数据节点形成一个数据流管道，将数据包分发到各个数据节点。  
+ 5.因为数据通过网络发送到不同机器上的各个数据节点，因此为了保证所有数据节点的数据都是准确的，接收到数据的数据节点要向发送者发送确认包。  
    确认包沿着数据流管道逆流而上依次经过各个数据节点并最终发往客户端，客户端接收到确认包后，会将对应的分包从内部队列中移除。  
    不断执行3 ~ 5步直至数据写完.  
+ 6.客户端调用FSDataOutputStream.close()方法关闭输出流。当DFSOutputStream对象内部队列中的分包都收到应答，就可用ClientProtocol.complete()方法通知名称节点关闭文件，完成一次正常写过程 
# HDFS命令
-R表示递归操作  

|命令|解释|
|---|---|
|hadoop fs -ls [-R][D]|显示目录下所有文件|
|hadoop fs -cat[file]|打开文件|
|hadoop fs -chgrp [-R] group [F/D]|更改文件/目录的所属组|
|hadoop fs -chown [-R] owner[:group] [F/D]|更改目录/文件所有者[所有组]|
|hadoop fs -chmod [-R]{ug0}{+-=}{rwx}[F/D]|更改文件目录权限|
|hadoop fs -tail [-f][F]||显示文件最后1KB内容，-f持续监测新添加的内容|
|hadoop fs -stat [format] [F/D]|显示文件/目录信息，不加format返回文件创建日期|
|hadoop fs -touchz [F]|创建文件|
|hadoop fs -mkdir [-p] [D]|创建目录|
|hadoop fs -copyFromLocal <localdst> <src>|从本地复制文件/目录到hadoop文件系统中|
|hadoop fs -copyToLocal <src> <localdst>|从hadoop文件系统中复制文件/目录到本地|
|hadoop fs -cp[SD/SF][DD]|复制文件|
|hadoop fs -du [F/D]|显示指定文件及文件夹中所有文件大小|
|hadoop fs -expunge|清空回收站|
|hadoop fs -get <src> <localdst>|从hadoop文件系统中复制文件/目录到本地|
|hadoop fs -getmerge[-nl]<src><localdst>||从hadoop文件系统中合并文件写入到本地，-nl在每个文件结尾添加一个换行符|
|hadoop fs -put <localdst> <src>|从本地复制文件/目录到hadoop文件系统中|
|hadoop fs -moveFromLocal <localdst> <src>|从本地复制文件/目录到hadoop文件系统中，删除本地文件|
|hadoop fs -mv <src> <dst>||
|hadoop fs -rm [F]|删除文件|
|hadoop fs -rm -r [D]|删除目录|
|hadoop fs -test [-zed] [F/D]|检验文件是否为存在，空，为目录|
|hadoop fs -setrep [-R] [F]|设置文件副本系数|
|hadoop fs -text [F]|将文件指定输出为文本格式|

# HDFS API的使用
  
```java

```
  
# 补充:数据均衡
数据创建，删除和磁盘存储饱和导致数据分配不平衡，从而可能导致map任务分配空的数据节点，进而导致网络带宽的消耗，此时需要平衡数据的分配。
## 原则
+ 数据平衡不能导致数据块减少，数据块备份丢失  
+ 管理员可以中止数据平衡进程  
+ 每次移动的数据量以及占用的网络资源，必须是可控的  
+ 数据均衡过程，不能影响namenode的正常工作  
## 过程
+ 数据均衡服务（Rebalancing Server）首先要求 NameNode 生成 DataNode 数据分布分析报告,获取每个DataNode磁盘使用情况  
+ Rebalancing Server汇总需要移动的数据分布情况，计算具体数据块迁移路线图。数据块迁移路线图，确保网络内最短路径  
+ 开始数据块迁移任务，Proxy Source Data Node复制一块需要移动的数据块  
+ 将复制的数据块复制到目标DataNode上  
+ 删除原始数据块  
+ 目标DataNode向Proxy Source Data Node确认该数据块迁移完成  
+ Proxy Source Data Node向Rebalancing Server确认本次数据块迁移完成。然后继续执行这个过程，直至集群达到数据均衡标准  
  
  
在第2步中，HDFS会把当前的DataNode节点,根据阈值的设定情况划分到Over、Above、Below、Under四个组中。在移动数据块的时候，Over组、Above组中的块向Below组、Under组移动。  
![contents](https://github.com/MzjHarley/Big-Data-Technology/blob/main/img/6.png)















