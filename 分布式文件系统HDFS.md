|Author|Date|
|---|---|
|MZJ|3/7,2022|

>HDFS，全称为"Hadoop Distributed File System",是一种通过网络实现文件在多台主机上进行分布式存储的文件系统，主要针对大规模数据的存储而设计的，用于处理大规模文件，如TB级  
>采用"客户机/服务器"模式，客户端以特定的通信协议通过网络与服务端建立连接，提出文件访问请求，客户端和服务器可以设置访问权限来限制请求方对底层数据存储块的访问。
## 计算机集群结构

分布式文件系统**将文件分布存储到多个计算机节点，成千上万个计算机节点构成计算机集群。**  
**目前分布式文件系统采用的计算机集群为普通硬件**。这就大大降低了硬件上的开销。  
集群中的计算机节点放在机架上，机架上可以存放8 ~ 64个节点，**同一机架的不同计算机节点之间通过网络互连，不同机架之间采用交换机或另一级网络互联**。  

![contents](https://github.com/MzjHarley/Big-Data-Technology/blob/main/img/2.png)

## 分布式文件系统的结构
分布式文件系统**将文件分成若干个块进行存储**，块是数据读写的基本单元，HDFS第一代默认一个块的大小为64MB，第二代默认128MB.  
与普通文件系统不同，在分布式文件系统中，如果一个文件小于一个数据块的大小，它并不占用整个数据块的存储空间。  
分布式文件系统的物理结构上由计算机集群中的多个节点构成。节点分为两类。
|节点类型|作用|
|---|---|
|主节点|也叫名称节点，负责文件和目录的创建，删除和重命名等，同时管理着数据节点和文件块的映射关系（元数据）|
|从节点|也叫数据节点，负责存储和读取数据，据名称节点的命令创建，删除和复制数据块|

#![contents](https://github.com/MzjHarley/Big-Data-Technology/blob/main/img/4.png)

>客户端只有通过**访问名称节点才能找到请求的文件块所在位置，进而到相应位置读取所需文件块。**  
>在存储时，由名称节点分配存储位置，然后由客户端把数据写入相应数据节点。  
>在读取时，客户端从名称节点获得数据节点和文件块的映射关系，到相应位置访问文件块。  
>由于计算机集群上的节点可能发生故障，为保证数据的完整性，分布式文件系统通常采用多副本存储。  
>文件块会被复制成多个副本，存储在不同的节点上，而不同副本的各个节点会分布在不同的机架上。  
>这样在单个节点出现故障时，可以快速调用副本重启单个节点上的计算过程，而不用重启整个计算过程。整个机架出现故障时不会丢失所有文件块。
## 分布式文件系统的设计需求
|设计需求|具体含义|HDFS实现情况|
|----|----|----|
|透明性|  包括访问透明性、位置透明性、性能和伸缩透明性；访问透明性：用户不需要专门区分哪些是本地文件，哪些是远程文件。用户能够通过相同的操作来访问本地和远程文件资源；位置透明性：不改变路径名的前提下，不管文件副本数量和实际存储位置发生何种变化，对用户而言都可以通过相同的路径名访问同一个文件；性能和伸缩透明性：系统中节点的增加和减少以及性能的变化对用户而言是透明的，用户感觉不到什么时候节点加入或退出| HDFS 只能提供一定程度的访问透明性，完全的位置透明性、性能和伸缩透明性|
|并发控制|同一时间同一文件只允许一个writer上传|任何时间只允许一个程序写入某个文件|
|文件复制|一个文件可在不同位置拥有多个副本|多副本机制|
|硬件和操作系统异构性|可在不同的计算机和OS上实现同样的客户端和服务端程序|采用java语言开发|
|可伸缩性|支持文件的动态加入或退出|建立在大规模廉价机器上的分布式文件系统集群具有很好的伸缩性|
|容错|保证文件服务在服务端或用户端出现问题时能正常使用|多副本机制，故障自动检测和自动恢复机制|
|安全|保障系统安全性|安全性较弱|
## HDFS特点
硬件出错在普通服务器集群中是一种常态并非异常，因此HDFS在设计上采取了多种机制保证硬件出错的情况下数据的完整性。
|特点|机制|
|---|---|
|兼容廉价的硬件设备|快速检测硬件故障和进行自我恢复的机制|
|流式数据读写|HDFS是为了满足批量数据处理的要求而设计的，因此为了提高数据的吞吐率，放松了POSIX（可移植操作系统接口标准）的要求，从而以流式方式访问文件系统数据，即通过API打开文件的某个数据块后，能够顺序读取；或者通过API向HDFS写入数据。我觉得可以这样理解，就是通过java流式读取或写入。|
|大数据集|HDFS文件通常可以达到GB甚至TB级别|
|简单的文件模型|一次写入，多次读取，文件一旦完成写入，关闭后无法在写入，只能被读取|
|强大的跨平台兼容性|采用java实现|
|不适合低延迟数据访问|HDFS主要针对大规模数据批量处理而设计的，采用流式数据读取，具有很高的数据吞吐率，也意味着较高的延迟|
|无法高效存储大量小文件|小文件是指文件大小小于一个块的文件|
|不支持多用户写入及任意修改文件|HFDS只允许一个文件有一个写入者，不允许多个用户对同一文件执行写操作，而且只允许追加，不可执行随机写操作|
### 为什么HDFS无法高效存储大量小文件？
+ HDFS采用名称节点管理文件系统的元数据，这些元数据被保存在内存中，从而使客户端可以快速获取文件实际存储位置。  
  如果存储大量的小文件，那么名称节点就要消耗很多的内存来保留小文件的元数据，这样元数据检索的效率就会降低，需要花费很多时间找到一个文件的存储位置。  
  如果再继续扩展到十数亿个小文件，名称节点保存元数据所需要的内存空间就会大大增加，以现有硬件水平，无法在内存中保留如此大量的元数据。  
+ 用MapReduce处理大量小文件时，会产生很多的Map任务，进程管理开销大大增加。  
+ 访问小文件的速度要远远高于访问大文件的速度，因为访问大量小文件需要不断从一个节点跳到另一个节点，严重影响性能。  

# HDFS相关概念
## 块
传统文件系统为了提高磁盘的读写效率，一般以数据块为单位，并非字节。以块为单位读写数据，可以把磁盘的寻道时间分摊到大量数据中，一旦找到第一条记录，剩下的顺序读取效率是很高的。  
HDFS同样也采用了块的概念，默认一个块为64MB.
### HDFS的块要比传统文件系统的块大得多，为什么？
+ 最小化寻址开销。HDFS的寻址开销包括磁盘寻道开销和数据块定位开销。  
  当客户端访问一个文件时，首先要从名称节点获取到组成这个文件的数据块的位置列表，然后根据这个位置列表获得实际存储的数据节点的位置，最后数据节点根据数据块信息在本地linux文件系统中找到 对应的文件，并把数据返回给客户端。  
  设计一个比较大的块，可以将寻址开销分摊到较多的数据中，降低单位数据的寻址开销。  
+ 块的大小不宜过大，MapReduce中的Map任务一次只处理一个数据块，如果启动的任务太少，会降低作业并行处理速度。
### HDFS采用块好处
+ 支持大规模文件存储。文件以块为单位进行存储，文件被分成多个文件块存储到不同数据节点上，使得一个文件的大小不受制于单个节点容量的限制可以远远大于网络中任意节点的存储容量。  
+ 简化系统设计。首先大大简化了存储管理，文件块大小固定，可以容易的计算出一个节点可以存储多少文件块，其次方便元数据管理，元数据不需要和文件块一起存储而由其它系统管理。    
+ 适合数据备份。每个文件块都可冗余存储到多个节点，大大提高系统的容错性和可用性。
## 名称节点和数据节点
**在HDFS中，名称节点负责管理文件系统的命名空间，保存了两个核心的数据结构FsImage和EditLog。**  
**FsImage用于维护文件系统树以及文件系统中所有文件和文件夹的元数据；EditLog记录了所有针对文件的创建/删除/重命名等操作。**   
**名称节点记录了每个文件中各个块所在数据节点的位置信息，但并不持久地存储这些信息，而是在系统启动时扫描所有数据节点并重构，得到这些信息。** 
名称节点**在启动时会将FsImage中的内容加载进内存中，然后执行EditLog中的各项操作，使内存中的元数据保持最新，这个操作完成后就会产生新的FsImage文件和EditLog文件**。  
名称节点启动后并进入到正常运行状态后，HDFS中的所有更新操作都会写入到EditLog文件而非FsImage文件。这是因为对于分布式文件系统而言，**FsImage文件很大（GB级别以上），如果所有更新操作都写入到FsImage中会使系统的运行速度非常缓慢。相对来说EditLog通常要远远小于FsImage，更新操作写入到EditLog是非常高效的。**  
名称节点在启动时会进入安全模式只能提供读操作无法提供写操作，启动结束后进入正常运行状态可对外提供读写操作。  
数据节点：是HDFS中的工作节点，负责数据的读取和存储，会根据客户端或者名称节点的调度进行数据的存储和检索，并且会定期向名称节点发送自己所存储块的列表信息。每个数据块中的数据都会被保存在本地Linux文件系统中。  
## 第二名称节点
在名称节点运行期间，HDFS会不断产生更新操作，致使EditLog文件不断变大，不断变大的EditLog通常对系统的性能不会产生显著影响。  
但当名称节点重启时，需要将FsImage文件加载进入内存并执行EditLog文件中的所有操作，使FsImage文件保持最新，可想而知如果EditLog文件过于大的话，名称节点就会长时间处于安全模式，无法对外提供写操作，从而影响用户使用。  
为有效解决这个问题，HDFS在设计中采用了第二名称节点。**主要具有两个功能：完成EditLog和FsImage的合并操作，减小EditLog的大小，缩短名称节点的重启时间；作为名称节点的检查节点，保存名称节点中的元数据信息。**  
### FsImage和EditLog的合并操作。
+ 每隔一段时间，第二名称节点会和名称节点通信，请求停止EditLog文件的使用(这个时刻记作t1)，暂时将新到达的写操作添加到一个新的文件EditLog.new。  
+ 然后，第二名名称节点会将名称节点中的FsImage和EditLog文件拉回本地，再加载到内存中，对二者执行合并操作(在内存中逐步执行EditLog文件中的操作，使FsImage文件保持最新)。  
+ 合并结束后，第二名称节点会将最新的FsImage.ckpt文件发送给名称节点。  
+ 名称节点收到后，会用最新的FsImage.ckpt文件去替换旧的FsImage文件，同时用最新的EditLog.new文件替换旧的EditLog文件。(这个时刻记作t2)从而减少EditLog的大小。  
### 作为名称节点的检查点
从合并过程中可以看出第二名称节点会定期和名称节点进行通信，从名称节点获取FsImage和EditLog文件，执行合并操作后得到新的FsImage.ckpt文件。  
从这个角度可以看出第二名称节点相当于为名称节点设置了一个检查点，周期性的备份名称节点中的元数据。  当名称节点发生故障时，就会用第二名称节点中记录的元数据信息进行系统恢复。  
但第二名称节点上合并操作得到的新的FsImage文件是合并发生时(t1时刻)HDFS得元数据信息，并未包含t1 ~ t2间发生的更新操作。如果名称节点在t1 ~ t2期间发生故障，系统就会丢失部分元数据信息。  
HDFS并不支持把系统直接切换到第二名称节点，从这个角度来讲，第二名称节点只是起到“检查点”的作用，并非“热备份”的作用。  
# HDFS体系结构
HDFS采用主从结构模型，一个HDFS集群包含一个名称节点和若干个数据节点。**名称节点作为中心服务器负责管理文件系统的命名空间及客户端对文件的访问**。  
集群中的数据节点一般是一个节点运行一个**数据节点**的进程，**负责处理客户端的读写请求，在名称节点的统一调度下进行数据块的创建，删除和重命名等操作**。  
每个数据节点实际是保存在本地linux文件系统下的。他们会定期向名称节点**发送心跳信息(自己所存储块的列表信息)，报告自己的状态，没有按时发送心跳信息的数据节点会被标记为死机状态，名称不会再给分配任何I/O请求**。    
当客户端访问一个文件时，首先将文件名发送给名称节点，名称节点根据文件名找到对应数据块的位置信息，然后根据每个数据块的信息找到实际存储各个数据块的数据节点的位置，将数据节点位置发送给客户端，最后客户端直接访问这些数据节点获取数据。**在整个访问过程中，名称节点并不参与数据的传输，这种设计方式可以实现在不同数据节点上的并发访问，大大提高访问速度**。  
HDFS采用java语言开发，具有很强的跨平台兼容性，因此任何支持JVM的机器都可部署名称节点和数据节点。实际部署时，会选用一台性能较好的机器作为名称节点，其他机器作为数据节点。当然一台机器可以运行多个数据节点，名称节点也可以和数据节点放在同一台机器中运行。  
HDFS集群中只有一个名称节点，该节点负责所有元数据的管理，这种设计大大简化了分布式文件系统的结构，可以保证数据不会脱离名称节点的控制，同时用户的数据永远不会经过名称节点，大大减轻了名称节点(中心服务器)的负担，方便数据管理。  
## HDFS命名空间管理
命名空间包括文件，块和目录，存储在本地linux文件系统中。  
命名空间的管理是指命名空间支持对HDFS中的目录，文件和块做类似于一般文件系统的创建，修改和删除等基本操作，在HDFS体系结构中，只有一个命名空间。  
HDFS使用的是传统的分级文件体系，因此，用户可以像使用普通文件系统一样，创建、删除目录和文件，在目录间转移文件，重命名文件等。  
## 通信协议
所有HDFS通信协议都是构建在TCP/IP基础上的。  
名称节点不会主动发起RPC，而是响应来自客户端和数据节点的RPC请求。  
![contents](https://github.com/MzjHarley/Big-Data-Technology/blob/main/img/5.png)
## 体系结构的局限性
+ 命名空间的限制，名称节点保存在内存中，因此名称节点能够容纳对象的个数受到内存空间大小的限制。  
+ 性能的瓶颈，整个分布式文件系统的吞吐量受限于单个名称节点的吞吐量。  
+ 隔离问题，集群中只有一个名称节点和命名空间，无法对不同程序进行隔离。  
+ 集群可用性，一旦唯一的名称发生故障，会导致整个集群不可用。
# HDFS的存储原理
## 数据冗余存储
+ 加快数据传输速度，当多个客户端同时访问同一文件时，可以让客户端分别从不同的数据块副本中读取数据，大大加快了数据的传输速度。  
+ 容易检查数据错误，数据节点之间通过网络传输数据，采用多副本可以很容易判断数据是否出错。  
+ 保证数据的可靠性，某个数据节点失效，也不会造成数据丢失。

## 数据存取策略
### 数据存放
为了充分提高数据的可靠性与系统的可用性以及充分利用网络带宽，HDFS采用以机架为基础的数据存放策略。  
一个HDFS集群中包含很多机架，机架内部通过网络互连，机架之间通过路由器或交换机互连，这意味着机架内部通信带宽大于机架之间的通信带宽。  

>HDFS默认每个数据节点在不同的（逻辑）机架上.  
>由于这个机架是通过Map映射来完成的，所以本质上是一个逻辑机架，也因此可以将不同物理机架上的节点配置在同一个逻辑机架上。  
>在实际开发过程中，为了方便管理，往往是将同一个物理机架上的节点配置在同一个逻辑机架上    
>缺点：不能充分利用同一机架内部机器之间的带宽；
>优点:获得很高的数据可靠性(即使一个机架发生错误，其余机架上的数据副本仍然是可用的)  
>     可以在多个机架上进行并行读取数据，大大提高数据读取速度。  
>     可以很容易实现系统内部负载均衡(数据分配不平衡，导致map任务分配空的数据节点，导致网络带宽的消耗，需要平衡数据的分配)和错误处理。    

|副本放置策略|
|---|
|1.集群内发送读写请求，第1个副本放在发起请求的数据节点上，实现就近写入数据|
|2.集群外发起读写请求，则从集群内部挑选一台磁盘较为充足,CPU不太忙的数据节点，作为第1个副本的存放地|
|3.第2个副本会被放在与第1个副本不同机架上的数据节点|
|4.第3个副本会放在和第1个副本相同机架的其他节点|
|5.如果还有更多副本，则会随机选择数据节点进行存放|
## 数据读取
HDFS为了降低整体的网络带宽消耗和数据读取延时，HDFS集群一定会让客户端尽量去读取近的副本，那么按照以上头解释的副本存放策略的结果：  
1、如果在客户端数据节点有数据，那么直接读取  
2、如果在跟客户端数据节点同机架的数据节点中有该数据块，则直接读取  
3、如果该HDFS集群跨多个数据中心，那么客户端也一定会优先读取本数据中心的数据  
HDFS是如何确定两个节点是否是同一节点，如何确定的不同服务器跟客户端的远近呢？答案就是机架感知。  
机架感知可以计算两个数据节点之间的距离，通过一个机架感知的过程，NameNode可以确定每一个DataNode所属的机架id.  
有了机架感知，NameNode就可以画出Datanode网络拓扑图,就可以计算出任意两台datanode之间的距离,得到最优的存放和读取策略,优化整个集群的网络带宽均衡以及数据最优分配。  
HDFS通过一个API可以确定一个数据节点所属机架ID ，客户端也可以调用API(机架感应)获取自己的所属机架ID.，读取原则是就近读取。  
## 数据复制
HDFS的数据采用了流水线复制的策略，大大提高了数据复制过程的效率。
### 当客户端向HDFS写入一个文件时
+ 这个文件首先被写入到本地，并被切分成若干个块，每个块的大小由HDFS的设定值决定。
+ 每个块都会向名称节点发起写请求，名称节点会根据系统内各个数据节点的使用情况，会返回一个数据节点列表给客户端。
+ 




















